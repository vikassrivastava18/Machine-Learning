{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2ceaeb-460a-4410-b2a9-f0ce713980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('style.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66126abc-c545-4493-964e-43cc466c04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fc0e813-9681-4446-b5c9-229e753a3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b207aaa-79b0-48c3-8161-d18d81ef2652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9804d-8143-4b5a-b6cc-8c83676bc543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfa539e-8ddf-40f3-89ad-6ca83ae69e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b     \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64c6132-cefb-4f9e-b10b-49623f9ef2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [852   2   1  35]\n",
      "f_wb shape (), prediction: 177.99999898940814\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[2,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98261831-d80c-4ad6-bf22-412c914d5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\"\n",
    "    Single predict using linear regression\n",
    "    Args:\n",
    "        x (ndarray): Shape (n,) example with multiple features\n",
    "        w (ndarray): Shape (n,) model parameters (weights)\n",
    "        b (scalar): model parameter (bias)\n",
    "    Returns:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f6ce6f-9ca4-419b-a317-b64f8bfb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute cost\n",
    "    Args:\n",
    "        X (ndarray (m, n)): Data, m examples with n features\n",
    "        y (ndarray (m,)): Target values\n",
    "        w (ndarray (n,)): model weights\n",
    "        b (scalar): model bias\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = np.shape(X)[0]\n",
    "\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb_i = predict(X[i], w, b)\n",
    "        cost += (f_wb_i - y[i]) ** 2\n",
    "    cost /= 2 * m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ddd887-f1c0-40ef-8fa2-9f4752403b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b2e46b-d3fe-4cdf-8db0-4143275c5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes gradient using: \n",
    "        dJ_dw: sum((predict(X_i, w, b) - y_i) * X_i) / m\n",
    "        dJ_db: sum(predict(X_i, w, b) - y_i) / m\n",
    "    \"\"\"\n",
    "    m, n = np.shape(X)\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "        err = (predict(X[i], w, b) - y[i])\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * X[i][j]\n",
    "        dj_db += err\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b799f9cc-fa21-4a3d-896e-4d3ecc9fe0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251501955248e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059f63a-d4b2-4363-91ef-2c5b9336d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb3ccd-a4a0-43c8-a764-7ab905014929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182df0b3-2d45-483e-aadf-a193ffa668c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ed5cd-5dac-4859-a257-dc28bde29468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
